import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,d as t,o as i}from"./app-DKM7O08a.js";const o={};function s(r,a){return i(),e("div",null,a[0]||(a[0]=[t('<h1 id="论文精读——srgan" tabindex="-1"><a class="header-anchor" href="#论文精读——srgan"><span>论文精读——SRGAN</span></a></h1><p>https://arxiv.org/pdf/1609.04802</p><h2 id="引入" tabindex="-1"><a class="header-anchor" href="#引入"><span>引入</span></a></h2><ol><li>在图像超分引入GAN网络</li><li>引入新的图像评价指标——利用VGG提取超分/原始图像特征，对这些特征进行比较</li><li>综合了GAN（判别器）+ 指标图像评价指标提出新的损失函数</li><li>MOS测试——并不简单基于MSE评价图像质量，更多考虑肉眼感知质量</li></ol><h2 id="abstract" tabindex="-1"><a class="header-anchor" href="#abstract"><span>Abstract</span></a></h2><figure><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104009.png" alt="20241028104009" tabindex="0" loading="lazy"><figcaption>20241028104009</figcaption></figure><h2 id="_1-introduction" tabindex="-1"><a class="header-anchor" href="#_1-introduction"><span>1. Introduction</span></a></h2><p><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104054.png" alt="20241028104054" loading="lazy"> PSNR高不代表肉眼感知质量更高，其余略</p><h3 id="_1-2-contribution" tabindex="-1"><a class="header-anchor" href="#_1-2-contribution"><span>1.2. Contribution</span></a></h3><figure><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028103058.png" alt="20241028103058" tabindex="0" loading="lazy"><figcaption>20241028103058</figcaption></figure><ol><li>训练了更深更好的SRResNet</li><li>提出SRGAN</li><li>提出MOS与新的损失函数</li></ol><h3 id="_1-1-related-work" tabindex="-1"><a class="header-anchor" href="#_1-1-related-work"><span>1.1. Related work</span></a></h3><p>传统图像超分技术等</p><h2 id="_2-method" tabindex="-1"><a class="header-anchor" href="#_2-method"><span>2. Method</span></a></h2><p><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104537.png" alt="20241028104537" loading="lazy"><br> $G_{θG}$: 生成器<br> $I_n^{LR}$: low_resolution image 低分图像<br> $I_n^{HR}$: high_resolution image 高分图像（原始图像）<br> $l^{SR}$: 损失函数</p><h3 id="_2-1-adversarial-network-architecture" tabindex="-1"><a class="header-anchor" href="#_2-1-adversarial-network-architecture"><span>2.1. Adversarial network architecture</span></a></h3><figure><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028105748.png" alt="20241028105748" tabindex="0" loading="lazy"><figcaption>20241028105748</figcaption></figure><h3 id="_2-2-perceptual-loss-function" tabindex="-1"><a class="header-anchor" href="#_2-2-perceptual-loss-function"><span>2.2. Perceptual loss function</span></a></h3><p><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028105856.png" alt="20241028105856" loading="lazy"><br> 图像内容loss + GAN网络对抗loss（加权求和）</p><h4 id="_2-2-1-content-loss" tabindex="-1"><a class="header-anchor" href="#_2-2-1-content-loss"><span>2.2.1 Content loss</span></a></h4><p>作者列举了两种：<br><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028110216.png" alt="20241028110216" loading="lazy"><br> 传统MSE<br><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028110248.png" alt="20241028110248" loading="lazy"><br> 利用VGG网络得到特征图，计算特征图间的欧氏距离</p><h4 id="_2-2-2-adversarial-loss" tabindex="-1"><a class="header-anchor" href="#_2-2-2-adversarial-loss"><span>2.2.2 Adversarial loss</span></a></h4><figure><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028110821.png" alt="20241028110821" tabindex="0" loading="lazy"><figcaption>20241028110821</figcaption></figure><h2 id="_3-experiments" tabindex="-1"><a class="header-anchor" href="#_3-experiments"><span>3. Experiments</span></a></h2><h3 id="_3-3-mean-opinion-score-mos-testing" tabindex="-1"><a class="header-anchor" href="#_3-3-mean-opinion-score-mos-testing"><span>3.3. Mean opinion score (MOS) testing</span></a></h3><p><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028111110.png" alt="20241028111110" loading="lazy"><br> 找志愿者用肉眼评判图像质量/分辨率</p><h2 id="_4-discussion-and-future-work" tabindex="-1"><a class="header-anchor" href="#_4-discussion-and-future-work"><span>4. Discussion and future work</span></a></h2><p><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028111730.png" alt="20241028111730" loading="lazy"><br> “We speculate that feature maps of these deeper layers focus purely on the content while leaving the adversarial loss focusing on texture details.<br> 我们推测，这些较深层的特征图纯粹关注内容，而将对抗性损失集中在纹理细节上。”<br> loss函数综合考虑了图像的整体内容和纹理细节。</p><h2 id="_5-conclusion" tabindex="-1"><a class="header-anchor" href="#_5-conclusion"><span>5. Conclusion</span></a></h2><p><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028112312.png" alt="20241028112312" loading="lazy"><img src="https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028112419.png" alt="20241028112419" loading="lazy"></p>',30)]))}const p=n(o,[["render",s],["__file","论文精读——SRGAN.html.vue"]]),g=JSON.parse('{"path":"/zh/posts/study/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94SRGAN.html","title":"论文精读——SRGAN","lang":"zh-CN","frontmatter":{"icon":"pen-to-square","date":"2024-10-25T00:00:00.000Z","category":["科研"],"tag":["论文精读","超分"],"description":"论文精读——SRGAN https://arxiv.org/pdf/1609.04802 引入 在图像超分引入GAN网络 引入新的图像评价指标——利用VGG提取超分/原始图像特征，对这些特征进行比较 综合了GAN（判别器）+ 指标图像评价指标提出新的损失函数 MOS测试——并不简单基于MSE评价图像质量，更多考虑肉眼感知质量 Abstract 2024...","head":[["meta",{"property":"og:url","content":"https://qby702301.github.io/blog/blog/zh/posts/study/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94SRGAN.html"}],["meta",{"property":"og:site_name","content":"QBY的博客"}],["meta",{"property":"og:title","content":"论文精读——SRGAN"}],["meta",{"property":"og:description","content":"论文精读——SRGAN https://arxiv.org/pdf/1609.04802 引入 在图像超分引入GAN网络 引入新的图像评价指标——利用VGG提取超分/原始图像特征，对这些特征进行比较 综合了GAN（判别器）+ 指标图像评价指标提出新的损失函数 MOS测试——并不简单基于MSE评价图像质量，更多考虑肉眼感知质量 Abstract 2024..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104009.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-10-28T05:46:20.000Z"}],["meta",{"property":"article:tag","content":"论文精读"}],["meta",{"property":"article:tag","content":"超分"}],["meta",{"property":"article:published_time","content":"2024-10-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-10-28T05:46:20.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"论文精读——SRGAN\\",\\"image\\":[\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104009.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104054.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028103058.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104537.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028105748.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028105856.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028110216.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028110248.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028110821.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028111110.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028111730.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028112312.png\\",\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028112419.png\\"],\\"datePublished\\":\\"2024-10-25T00:00:00.000Z\\",\\"dateModified\\":\\"2024-10-28T05:46:20.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"QBY\\",\\"url\\":\\"https://git.zju.edu.cn/3210103719\\"}]}"]]},"headers":[{"level":2,"title":"引入","slug":"引入","link":"#引入","children":[]},{"level":2,"title":"Abstract","slug":"abstract","link":"#abstract","children":[]},{"level":2,"title":"1. Introduction","slug":"_1-introduction","link":"#_1-introduction","children":[{"level":3,"title":"1.2. Contribution","slug":"_1-2-contribution","link":"#_1-2-contribution","children":[]},{"level":3,"title":"1.1. Related work","slug":"_1-1-related-work","link":"#_1-1-related-work","children":[]}]},{"level":2,"title":"2. Method","slug":"_2-method","link":"#_2-method","children":[{"level":3,"title":"2.1. Adversarial network architecture","slug":"_2-1-adversarial-network-architecture","link":"#_2-1-adversarial-network-architecture","children":[]},{"level":3,"title":"2.2. Perceptual loss function","slug":"_2-2-perceptual-loss-function","link":"#_2-2-perceptual-loss-function","children":[]}]},{"level":2,"title":"3. Experiments","slug":"_3-experiments","link":"#_3-experiments","children":[{"level":3,"title":"3.3. Mean opinion score (MOS) testing","slug":"_3-3-mean-opinion-score-mos-testing","link":"#_3-3-mean-opinion-score-mos-testing","children":[]}]},{"level":2,"title":"4. Discussion and future work","slug":"_4-discussion-and-future-work","link":"#_4-discussion-and-future-work","children":[]},{"level":2,"title":"5. Conclusion","slug":"_5-conclusion","link":"#_5-conclusion","children":[]}],"git":{"createdTime":1730085882000,"updatedTime":1730094380000,"contributors":[{"name":"QBY","email":"2403422539@qq.com","commits":2}]},"readingTime":{"minutes":1.49,"words":448},"filePathRelative":"zh/posts/study/论文精读——SRGAN.md","localizedDate":"2024年10月25日","excerpt":"\\n<p>https://arxiv.org/pdf/1609.04802</p>\\n<h2>引入</h2>\\n<ol>\\n<li>在图像超分引入GAN网络</li>\\n<li>引入新的图像评价指标——利用VGG提取超分/原始图像特征，对这些特征进行比较</li>\\n<li>综合了GAN（判别器）+ 指标图像评价指标提出新的损失函数</li>\\n<li>MOS测试——并不简单基于MSE评价图像质量，更多考虑肉眼感知质量</li>\\n</ol>\\n<h2>Abstract</h2>\\n<figure><img src=\\"https://qby-1330074694.cos.ap-nanjing.myqcloud.com/images/20241028104009.png\\" alt=\\"20241028104009\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>20241028104009</figcaption></figure>","autoDesc":true}');export{p as comp,g as data};
